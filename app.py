"""
–°–µ—Ä–≤–∏—Å –Ω–∞ Streamlit –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤ –æ–ø–æ—Ä –ì–¢–î
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º IsolationForest + feature engineering.
"""

import os
import pandas as pd
import numpy as np
import streamlit as st
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest

# -----------------------------------------------------------------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞–∫ –≤ analysis.ipynb)
# -----------------------------------------------------------------------------

DIAG_COLUMNS = [
    "V1", "Vo–ì–ì", "V2",
    "F—Ç–∫4", "Fc2", "Fc3", "Fc4",
    "F1", "2F1", "3F1", "F2", "2F2", "3F2", "F–∫–ø–∞", "F—Ü—Å",
    "Pm", "P615", "dPfgo", "dPf1", "dPf2",
    "–†—Å1", "–†—Å2",
    "T607", "T606", "T600", "T638", "T–∫.–∑.", "Lm1",
]

REGIME_COLUMNS = ["N1", "N2", "N3", "Qtg", "P2", "T1", "T4—Å—Ä"]

ALL_FEATURE_COLUMNS = DIAG_COLUMNS + REGIME_COLUMNS

# –ö–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∑–∞–≥—Ä—É–∂–∞–µ–º–æ–º —Ñ–∞–π–ª–µ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä)
REQUIRED_COLUMNS = [
    "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è",
    "N1", "N2", "N3", "Qtg", "P2", "T1", "T4—Å—Ä",
    "V1", "Vo–ì–ì", "V2",
    "F1", "2F1", "3F1", "F2", "2F2", "3F2",
    "F—Ç–∫4", "Fc2", "Fc3", "Fc4",
    "Pm", "P615", "dPfgo", "dPf1", "dPf2",
    "–†—Å1", "–†—Å2",
    "T607", "T606", "T600", "T638", "T–∫.–∑.", "Lm1",
]


def load_csv(file_or_path, is_upload=True) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ CSV —Å –æ–∂–∏–¥–∞–µ–º—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º (sep=';', decimal=',')."""
    try:
        if is_upload:
            df = pd.read_csv(
                file_or_path,
                sep=";",
                decimal=",",
                encoding="utf-8",
                dayfirst=True,
            )
        else:
            df = pd.read_csv(
                file_or_path,
                sep=";",
                decimal=",",
                encoding="utf-8",
                parse_dates=["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"],
                dayfirst=True,
            )
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        return None

    # –ü—Ä–∏–≤–æ–¥–∏–º "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è" –∫ datetime, –µ—Å–ª–∏ –µ—â—ë –Ω–µ
    if "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è" not in df.columns:
        st.error('–í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è".')
        return None

    if not pd.api.types.is_datetime64_any_dtype(df["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"]):
        df["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"] = pd.to_datetime(
            df["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"],
            dayfirst=True,
            errors="coerce",
        )

    return df


def make_feature_table(df: pd.DataFrame, window: int = 60) -> pd.DataFrame:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –∞–Ω–æ–º–∞–ª–∏–π."""
    df = df.sort_values("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è").reset_index(drop=True).copy()
    df = df.set_index("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è")

    cols = [c for c in ALL_FEATURE_COLUMNS if c in df.columns]
    if len(cols) < 10:
        return None
    feat = df[cols].astype(float)

    vib_keys = [
        c
        for c in [
            "Vo–ì–ì", "V1", "V2",
            "F1", "2F1", "3F1", "F2", "2F2", "3F2",
            "F—Ç–∫4", "Fc2", "Fc3", "Fc4",
        ]
        if c in feat.columns
    ]

    roll = feat[vib_keys].rolling(window=window, min_periods=window // 3)
    roll_mean = roll.mean().add_suffix("_roll_mean")
    roll_std = roll.std().add_suffix("_roll_std")

    ratio_features = {}
    if "F1" in feat.columns and "2F1" in feat.columns:
        ratio_features["F2F1_over_F1"] = feat["2F1"] / (feat["F1"] + 1e-6)
    if "F1" in feat.columns and "3F1" in feat.columns:
        ratio_features["F3F1_over_F1"] = feat["3F1"] / (feat["F1"] + 1e-6)
    if "F2" in feat.columns and "2F2" in feat.columns:
        ratio_features["F2F2_over_F2"] = feat["2F2"] / (feat["F2"] + 1e-6)
    if "F2" in feat.columns and "3F2" in feat.columns:
        ratio_features["F3F2_over_F2"] = feat["3F2"] / (feat["F2"] + 1e-6)

    ratio_df = pd.DataFrame(ratio_features, index=feat.index)

    norm_features = {}
    if "N1" in feat.columns:
        for c in vib_keys:
            norm_features[f"{c}_per_N1"] = feat[c] / (feat["N1"] + 1e-3)
    if "N2" in feat.columns:
        for c in vib_keys:
            norm_features[f"{c}_per_N2"] = feat[c] / (feat["N2"] + 1e-3)
    norm_df = pd.DataFrame(norm_features, index=feat.index)

    full = pd.concat([feat, roll_mean, roll_std, ratio_df, norm_df], axis=1)
    full = full.dropna().reset_index()
    return full


def check_columns(df: pd.DataFrame):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫."""
    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]
    return len(missing) == 0, missing


# -----------------------------------------------------------------------------
# Streamlit UI
# -----------------------------------------------------------------------------

st.set_page_config(
    page_title="–î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤ –ì–¢–î",
    page_icon="üîß",
    layout="wide",
)

st.title("üîß –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –ø–æ–¥—à–∏–ø–Ω–∏–∫–æ–≤ –æ–ø–æ—Ä –ì–¢–î")
st.markdown(
    "–ú–æ–¥–µ–ª—å: **IsolationForest** –Ω–∞ engineered-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (—Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, "
    "–æ—Ç–Ω–æ—à–µ–Ω–∏—è –≥–∞—Ä–º–æ–Ω–∏–∫, –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ –æ–±–æ—Ä–æ—Ç–∞–º)."
)

# -----------------------------------------------------------------------------
# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
# -----------------------------------------------------------------------------

@st.cache_resource
def fit_model(train_path: str):
    """–û–±—É—á–µ–Ω–∏–µ IsolationForest –∏ StandardScaler –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ."""
    df = load_csv(train_path, is_upload=False)
    if df is None or df.empty:
        return None, None, None, None, "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏."

    ok, missing = check_columns(df)
    if not ok:
        return None, None, None, None, f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–æ–ª–æ–Ω–æ–∫: {missing}"

    train_feat = make_feature_table(df, window=60)
    if train_feat is None:
        return None, None, None, None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ (–º–∞–ª–æ –∫–æ–ª–æ–Ω–æ–∫)."

    feature_cols = [c for c in train_feat.columns if c != "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"]
    X_train = train_feat[feature_cols].values

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    iso = IsolationForest(
        n_estimators=200,
        contamination=0.01,
        random_state=42,
        n_jobs=-1,
    )
    iso.fit(X_train_scaled)

    iso_scores_train = -iso.decision_function(X_train_scaled)
    thr = float(np.quantile(iso_scores_train, 0.995))

    return iso, scaler, feature_cols, thr, None


# –ü—É—Ç—å –∫ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
TRAIN_PATH = os.path.join(os.path.dirname(__file__), "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞.csv")
train_exists = os.path.isfile(TRAIN_PATH)

iso, scaler, feature_cols, thr, err = None, None, None, None, None

if train_exists:
    iso, scaler, feature_cols, thr, err = fit_model(TRAIN_PATH)
else:
    with st.expander("–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–≤–æ–π —Ñ–∞–π–ª.", expanded=True):
        train_upload = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É (CSV, sep=';', decimal=',')",
            type=["csv"],
            key="train_upload",
        )
        if train_upload:
            df_train = load_csv(train_upload, is_upload=True)
            if df_train is not None:
                ok, missing = check_columns(df_train)
                if not ok:
                    st.error(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–æ–ª–æ–Ω–æ–∫: {missing}")
                else:
                    train_feat = make_feature_table(df_train, window=60)
                    if train_feat is None:
                        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏.")
                    else:
                        _cols = [c for c in train_feat.columns if c != "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"]
                        X_tr = train_feat[_cols].values
                        _scaler = StandardScaler()
                        X_tr_scaled = _scaler.fit_transform(X_tr)
                        _iso = IsolationForest(
                            n_estimators=200,
                            contamination=0.01,
                            random_state=42,
                            n_jobs=-1,
                        )
                        _iso.fit(X_tr_scaled)
                        _scores = -_iso.decision_function(X_tr_scaled)
                        _thr = float(np.quantile(_scores, 0.995))
                        iso, scaler, feature_cols, thr = _iso, _scaler, _cols, _thr
                        st.session_state["iso"] = _iso
                        st.session_state["scaler"] = _scaler
                        st.session_state["feature_cols"] = _cols
                        st.session_state["thr"] = _thr
                        st.success("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ.")
        if "iso" in st.session_state and iso is None:
            iso = st.session_state["iso"]
            scaler = st.session_state["scaler"]
            feature_cols = st.session_state["feature_cols"]
            thr = st.session_state["thr"]

if train_exists and err:
    st.warning(f"**–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞:** {err}")

# -----------------------------------------------------------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
# -----------------------------------------------------------------------------

st.subheader("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
uploaded_file = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å —Ç–µ–º–∏ –∂–µ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, —á—Ç–æ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ "
    "(—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ;, –¥–µ—Å—è—Ç–∏—á–Ω–∞—è –∑–∞–ø—è—Ç–∞—è)",
    type=["csv"],
)

if uploaded_file and iso is not None and scaler is not None:
    df = load_csv(uploaded_file, is_upload=True)
    if df is not None:
        ok, missing = check_columns(df)
        if not ok:
            st.error(f"–í –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing}")
        else:
            feat = make_feature_table(df, window=60)
            if feat is None:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
            else:
                X = feat[feature_cols].reindex(columns=feature_cols, fill_value=0.0).values
                X_scaled = scaler.transform(X)
                scores = -iso.decision_function(X_scaled)

                feat["iso_score"] = scores
                feat["–∞–Ω–æ–º–∞–ª–∏—è"] = scores > thr

                n_total = len(feat)
                n_anom = int(feat["–∞–Ω–æ–º–∞–ª–∏—è"].sum())
                frac_anom = n_anom / n_total if n_total else 0

                st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {n_total} —Å—Ç—Ä–æ–∫. –ê–Ω–æ–º–∞–ª–∏–π: {n_anom} ({frac_anom:.2%})")
                st.metric("–ü–æ—Ä–æ–≥ (99.5% train)", f"{thr:.4f}")
                st.metric("–î–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π", f"{frac_anom:.2%}")

                tab1, tab2, tab3 = st.tabs(["–ì—Ä–∞—Ñ–∏–∫ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏", "–¢–æ–ø –∞–Ω–æ–º–∞–ª–∏–π", "–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"])

                with tab1:
                    import matplotlib.pyplot as plt

                    fig, ax = plt.subplots(figsize=(12, 4))
                    ax.plot(feat["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"], feat["iso_score"], label="iso_score")
                    ax.axhline(thr, color="r", linestyle="--", label=f"–ø–æ—Ä–æ–≥ {thr:.4f}")
                    ax.set_title("–ê–Ω–æ–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª IsolationForest –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
                    ax.set_ylabel("iso_score")
                    ax.set_xlabel("–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è")
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    plt.xticks(rotation=45)
                    st.pyplot(fig)
                    plt.close()

                with tab2:
                    top = feat.nlargest(100, "iso_score")[["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è", "iso_score", "–∞–Ω–æ–º–∞–ª–∏—è"]]
                    st.dataframe(top, use_container_width=True)

                with tab3:
                    st.dataframe(df.head(500), use_container_width=True)

elif uploaded_file and iso is None:
    st.error("–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –î–æ–±–∞–≤—å—Ç–µ '–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞.csv' –≤ –ø–∞–ø–∫—É –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.")
